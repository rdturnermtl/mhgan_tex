
title
% Do at end

abstract
% Do at end

Introduction

% GANs provide implicit way to do density estimation
% traditionally done with explicit likelihood, ancestral sampling in most models to get new synthetic data points from fit model
Generative adverserial networks (GANs) presented a radically new way to do density estimation:
They only implictly represent the density of the data via a classifier that distinguishes real from generated data.
Traditionally, density estimation of data has been done with model that can easily compute the likelihood of the observed data.
% TODO or if not, like RBM, could do MCMC method based on likelihood, GAN easy to sample but not likelihood

% GAN uses D and G, but then D usually thrown away
% Use new method to capture knowledge captured in D, to wrap G and create a more intelligent G'
GANs iterate between updating a discriminator $D$ and a generator $G$, where $G$ generates new samples of data, and $D$ attempts to distinguish samples of $G$ from the real data.
Typically, in this setup, $D$ is thrown away at the end of training, and $G$ is kept from generating new synthetic data points.
However, in this work we are able to construct a new generator $G'$ that ``wraps'' $G$ using the information contained in $D$.
% TODO include figure here

% TODO look into stats literature where G kept fixed and then D used to define density
% We combine both approaches by using D, but also learning a powerful G like in GANs

% Approach is based on notion that we can sample from real distn P_R implied by D that discriminates between P_R
% Corrolary is that if D is perfect, but G is good, we can sample from the data generating distn exactly
In this work we use Markov chain Monte Carlo (MCMC) methods to sample from the distribution implicitly defined by the discriminator $D$.
This is based on the notion that the discriminator classifies between the generator $G$ and a real data distribution:
\begin{align}
  D(x) = \frac{P_D(x)}{P_D(x) + P_G(x)}\,,
\end{align}
where $P_G$ is the (intractable) density of samples from the generator $G$, and $P_D$ is the real data density implied by the discriminator $D$.
If GAN training reaches its Nash equilibrium then this discriminator distribution is equal to the true distribution on the data ($P_D = P_R$)\@.  % TODO cite

% We use MCMC independence sampler to get samples from P_R using multiple samples from G
% MCMC not directly applicable since the likelihood of P_R and G are not available
% However, MCMC only needs the ratio of P_R/G and this can be found from D
We use an MCMC independence sampler to get samples from $P_D$ using multiple samples from $G$ (as the proposal)\@.
A corrolary of this is if given a perfect discrimantor $D$ and decent (but imperfect) generator $G$ we can obtain exact samples from the true data distribution $P_R$.
Standard MCMC implementations cannot acheive this because MCMC requires (unnormalized) densities for the target $P_D$ and the proposal $P_G$.
Neither of these quantities are available in GANs.
However, only strictly requires the ratio $P_D / P_G$, which is implied by the discriminator $D$.

% We also avoid the burn in issues usually associated with MCMC
% Typically, we cannot get a good initial sample because sampling over parameter space from posterior
% But in this application we are sampling from data space
% So, if init from true data point, then already at true distn for init, and will stay at true distn as result of detailed balance
% TODO do VAEs make similar kinds of arguments?
We can also avoid the burn-in issues that usually plague MCMC methods.
Recall that via the detailed balance property, if the marginal distribution on a Markov chain is at time step $k$ matches the target $P_D$ then the marginal at time step $k+1$ will also follow $P_D$.
Typically, it is not possible to get this initial sample from the target distribution becuase MCMC is usually applied to generator samples of parameters from a Bayesian posterior distribution.
These posteriors are strange objects that we have no idea how to sample from.
However, in this case, we are sampling from the data distribution.
Therefore, by initializing the chain at a sample of real data, we are already initializing it from the correct distribution.

% TODO usual outline of following sections, and bullet point contributions
% Could also hint at calibration, and cherry pick approx
We also derive a simple deterministic approximation to MCMC steps that is very accurate in the regimes that GANs operate in.
An important aspect of using the discriminator in MCMC is to have well calibrated probalities in $D$.
Typical GAN usage does not directly use $D$ and can survive poorly calibrated discriminators, however these probabilities become curcial for accept/reject probabilities.
We use a held out set recalibration routine for the MCMC GAN setup; and show that indeed the raw probabilities from $D$ in typical GAN training are poorly calibrated.

Background

% Short page limit so let's just get on same page with notation here.
In this section we quickly review the notation and equations with MCMC and GANs.

MCMC

MCMC methods attempt to draw samples from a target distribution $\vec x \sample \target$.
The Markov chain in MCMC works by first drawing a first sample from an initial distribution $\vec x_0 \sample \pinit$.
In general, the proposed succesive points are drawn from a conditional distribution $\vec x' \sample \prop(\vec x' \given \vec x_{k-1})$ (the proposal distribution)\@.
However, in this work we are using an independence sampler, which means $\vec x_k \sample \prop(\vec x_k)$.
The proposal $x'$ is accepted with probability:
\begin{align}
  \alpha = \min(1, \frac{\target(x')\prop(\vec x_{k-1} \given \vec x')}{\target(\vec x_{k-1})\prop(\vec x' \given \vec x_{k-1})})\,,
\end{align}
which for an independence sampler is:
\begin{align}
  \alpha = \min(1, \frac{\target(x')\prop(\vec x_{k-1})}{\target(\vec x_{k-1})\prop(\vec x')})\,.
\end{align}
If the point $x'$ is accepted then $x_k = x'$, otherwise $x_k = x_{k-1}$.
Note that when estimating the distribution on $\target$ one must include the duplicates that are a result of rejections in $x'$.

The detailed balance condition implies that if $x_{k-1} \sample \target$ exactly then $x_k \sample \target$ exactly as well.
Additionally, even if $x_k$ is not exactly distributed according to $\target$, its KL divergence to $\target$ will always decrease as $k$ increases.  % TODO cite

% also include not on bias from stopping rules
In our setup, in order to obtain independent samples, we sample $\vec x \sample \target$ and then run the chain a fixed number of iterations $K$ to obtain the final sample produced by the ``wrapped GAN'' $G'$.
Note that the number of samples $K$ is meant to be ancilliary to the state of the Markov chain $\vec x_k$, using the state of the Markov chain to determine when to stop has the potential to introduce bias.  % TODO cite

GANs

GANs work by modeling the distribution on data $\vec x \sample P_R$ by sampling synthetic data points from a generator; we use $P_R$ to refer to the unknown true distribution on the data $\vec x$.
The generator works by taking a sample from a Gaussian $\vec z \sample \norm(0,\mat I)$ then transforming it with a generator network $G$: $\vec x = G(\vec z)$.
This implies a distrubtion on the data $\vec x \sample P_G$.  % TODO make lower
However, because $G$ is often a complex and non-invertible function $P_G$ is generally intractable to compute.
% TODO mention realNVP

The discriminator function $D$ is merely a standard binary classifier, that gives the probability a data point is real (as opposed to being sampled from $P_G$)\@.
The training of a GAN interleaves updates where $D$ is trained as a standard classifier to maximize its performance classifier real samples from generated samples; and $G$ is trained to minimize the performance of $D$.
Therefore, GAN training forms a game between $D$ and $G$.
The original work of~\citet{} showed that if this training game converges to its Nash equilibrium, then
\begin{align}
  P_G = P_R,\, \quad D = \frac{P_R}{P_R + P_G} = \frac{1}{2}\,.
\end{align}
% TODO Cite
In practice, it is often found that it is much easier to get $D$ close to its Bayes optimal solution ($\frac{P_R}{P_R + P_G}$) than $G$ close to the true distribution $P_R$.
This motivates our approach of wrapping an imperfect $G$ to obtain an improved $G'$ using a near perfect $D$.

Related Work

% Need to be clear on diff with NICE-MCMC
There have been a few other works that in some way combine GAN training and MCMC methods.
The work of~\citept{} uses a GAN-like training procedure to improve the proposal used in MCMC to sample from an externally provided target density $\target$.
The use a RealNVP network as the proposal as intvertability is important for computation of the acceptance probability $\alpha$.
The different with this work is best summarized as:~\citet{} uses GANs to accelerate MCMC whereas we use MCMC to enhance the solution from a GAN.
% Other workshop paper too
% http://bayesiandeeplearning.org/2017/papers/6.pdf
A similar approach to~\citet{} was taken in~\citet{}, but with respect to finding improved proposals for particle filters as opposed to MCMC\@.

Methods

% Derive basic trick for MCMC using D
In this section we show how to sample from the distribution $P_D$ implied by the discriminator $D$.
If we assume that $D$ is an optimal discriminator for \emph{some} between the generator and some alternative distribution $P_D$, then:
\begin{align}
  D = \frac{P_D}{P_D + P_G} \implies \frac{P_D}{P_G} = \frac{1}{D^{-1}-1}\,, \label{eq:PD def}
\end{align}
and if $D$ is perfect then $P_D = P_R$.
We can use an MCMC independence sampler with a target distribution of $P_D$ and a proposal distribution of $P_G$.
This gives an acceptance probability of:
\begin{align}
  \alpha = \min(1, \frac{P_D(x')P_G(\vec x_{k-1})}{P_D(\vec x_{k-1})P_G(\vec x')})
    = \min(1, \frac{P_D(x')}{P_G(\vec x')} \frac{P_G(\vec x_{k-1})}{P_D(\vec x_{k-1})})
    = \min(1, \frac{D(\vec x_{k-1})^{-1} - 1}{D(\vec x')^{-1} - 1})\,, \label{eq:alpha from D}
\end{align}
which is computable using only the discrimator $D$ and no densities.

A key element to this is \emph{calibration}: The probalities for merely $D$ must not merely provide a good AUC score, but be onthe right scale.
Put in other terms, if one where to warp the probabilities of the perfect discriminator in~\eqref{eq:PD def} it may still suffice for standard GAN training, but it will not work in the MCMC procedure defined in~\eqref{eq:alpha from D}.
To test the calibration of the discriminator, we use a calibration test statistic defined in~\citet{} on a \emph{held out} data set:  % TODO cite Dawid
\begin{align}
  % TODO decide on notation N for size of calib set
  % np.sum(y_true - y_prob) / np.sqrt(np.sum(y_prob * (1.0 - y_prob)))
  Z := \frac{\sum_{i=1}^N y_i - D(\vec x_i)}{\sqrt{\sum_{i=1}^N D(\vec x_i) (1 - D(\vec x_i))}}\,.
\end{align}
If $D$ is well calibrated, i.e., $y$ is indistinguishable from a $y \sample D(\vec x)$, then $Z \sample \norm(0,1)$.
This means that large magnitute values for $Z$, i.e. ($|Z| > 2$), reject the hypothesis that $D$ is well-calibrated.
% TODO cite each of these.
To correct any miscalibration we then use the held out calibration set to calibrate $D$ using either logistic regression, isotonic regression, or beta calibration.

The other key aspect in any MCMC chain procedure is how one initializes the first sample $\vec x_0$.
In this process we can initialize $\vec x_0$ with a random draw of real data (ideally held out)\@.
Now, marginally $\vec x_0 ~ P_R$ exactly, and if $D$ is perfect, then by detailed balance $\vec x_k ~ P_R$ exactly as well, even if $P_G \neq P_R$.
However, if the generator $G$ is too poor we will never receive a proposal sample $\vec x'$ good enough to accept and replace the real data sample $\vec x_0$ with a generated one.
Another interesting note is that we do not actually need to store real data samples to for this initialization procedure.
Since~\eqref{eq:alpha from D} is merely a function of the discriminator scores, one must merely take a sample from the distribution on $D(\vec x)$ where $\vec x \sample P_R$.
This score $D(\vec x_0)$ forms a ``bar raiser'' that $D(\vec x')$ must come close to, but not necesarily exceed, to be accepted.

% Maybe discuss using side discriminator, but only is adds anything

% Cherry picking approximation
It is also possible to replace the Markov chain with a deterministic approximation that is more parallelizable.
% TODO also consider putting in the plot in logit scale
In Figure~\ref{} we show the acceptance probabilities for different levels of the current state $D(\vec x_k)$ and the proposal $D(\vec x')$.
The qualitative behavior of the curves are easily summarized.
In all curves, if $D(\vec x') \geq D(\vec x_k)$ then $\vec x'$ is always accepted.
As $D(\vec x_k) \rightarrow 0$ the region where $D(\vec x') < D(\vec x_k)$ becomes a linear ramp; and as $D(\vec x_k) \rightarrow 1$ approaches a step function.
% TODO cite arjovsky stuff
If $P_G$ and $P_R$ are well seperated (which believed to often be the case in GANs), and $D$ is near perfect, then $D(\vec x_0) \approx 1$ when the acceptance probability becomes a step function if $x_0 \sample P_R$.
Therefore, $D(\vec x_k)$ will stay near one and the acceptance function will be step function like.
In this scenario, we can approximate, $\alpha \approx \I{D(\vec x') > D(\vec x_k)}$, and therefore $G'$ takes the sample in the chain with the maximum score $D$.
But with the provision, that have the target to beat the score of a randomly chosen sample of real data $D(\vec x)$.
This maximum procedure is a reduction and is therefore easily parallelizable.

% Maybe Importance sampling also an option to include

Results

% Toy example with two gauss, std example with D and G
%   Show MH at diff num epochs

% Look at losses for discriminator
%    maybe rescale all loses to 0-1 scale for plotting

% Method taken for dealing with non-accepts

% Look at incep score over 3 method on 3 data sets per epoch
% Can also show results by MH iteration
% DC GAN, WGAN, PGAN
% Use JY scatter plot

% Look at distn on D to make look more like real
%   both on synth and real

% Show how calib needed in scores, and calib test

% Qualitative examples

Discusion

% How many MH iters usually needed, what about computation burden

% What does cherry picking perf say about distns in question
%    examples for resampling when overlap and no overlap
%    how it effects cherry pick approx

% Implications on how it can be applied more generally
%   Will be most interesting cases where might be some overlap

% Generally interesting notes on calibration and side disc
%   And that D is always so good

% Might need to consider stopping rule

conclusions
% usual wrap up BS paragraph




